{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find out outlier data by field\n",
    "\n",
    "# Iterate over features and create box plots\n",
    "# for feature in FeatureFieldN + NumberFields:\n",
    "for feature in [\"Age\"]:\n",
    "    print(feature)\n",
    "    df_orig[[feature]].boxplot()\n",
    "    plt.title(f'Distribution of {feature} Features')\n",
    "    plt.ylabel('Values')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to show all fields correlation\n",
    "correlation_matrix = df.corr()\n",
    "plt.figure(figsize=(40, 40))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='viridis', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To find out if it is possible to group loan application to approve/reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use unsuperivised kmeans method to group data\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# to extract features\n",
    "df_train = df[FeatureFieldNH + FeatureFieldOH + FeatureFieldB + NumberFields + [\"MonthlyLoanPayment\", \"LoanApproved\"]].head(10000)\n",
    "X = df_train.drop([\"LoanApproved\"], axis=1)\n",
    "\n",
    "# to standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# To apply K-Means clustering, for two clusters only, approve/reject\n",
    "best_score = -1\n",
    "best_labels = None\n",
    "best_model = None\n",
    "b = pd.DataFrame()\n",
    "# random_state = 58 # 58 is best for full dataset, 48 is best for first half dataset\n",
    "for r in range(40,70):\n",
    "    kmeans_model = KMeans(n_clusters=2, init='k-means++', random_state=r)\n",
    "    labels = kmeans_model.fit_predict(X_scaled)\n",
    "    # score = silhouette_score(X_scaled, labels)\n",
    "    b[\"LoanApprovedCompare\"] = df_train[\"LoanApproved\"] == labels.astype('bool')\n",
    "    score = b[\"LoanApprovedCompare\"].value_counts()[True] / b[\"LoanApprovedCompare\"].value_counts()[False]\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_labels = labels\n",
    "        best_model = kmeans_model\n",
    "        print(f\"best random_state={r} -> score={score}\")\n",
    "    else:\n",
    "        print(f\"random_state={r} -> score={score}\")\n",
    "\n",
    "# to save the trained model\n",
    "dump(best_model, 'kmeans_model.joblib')\n",
    "\n",
    "# To add cluster labels to the original DataFrame\n",
    "C = X\n",
    "C['LoanApproved'] = df_train[\"LoanApproved\"]\n",
    "C['LoanApprovedAI'] = best_labels.astype('bool')\n",
    "C[\"LoanApprovedCompare\"] = C[\"LoanApproved\"] == C[\"LoanApprovedAI\"]\n",
    "\n",
    "C[\"LoanApprovedCompare\"].value_counts()[True] / C[\"LoanApprovedCompare\"].value_counts()[False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to re-use the model\n",
    "\n",
    "# to load the model from file\n",
    "new_model = load('kmeans_model.joblib')\n",
    "# to predict the result\n",
    "new_labels = new_model.predict(X_scaled)\n",
    "\n",
    "C = X\n",
    "C['LoanApproved'] = df_train[\"LoanApproved\"]\n",
    "C['LoanApprovedAINew'] = new_labels.astype('bool')\n",
    "C['LoanApprovedAI'] = best_labels.astype('bool')\n",
    "C[\"LoanApprovedCompare\"] = C[\"LoanApproved\"] == C[\"LoanApprovedAI\"]\n",
    "C[\"LoanApprovedAICompare\"] = C[\"LoanApprovedAI\"] == C[\"LoanApprovedAINew\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logic before change to Loan Approved Seed\n",
    "# to use unsupervised Gaussian Mixture method to train the model\n",
    "\n",
    "# to construct training data\n",
    "df_train = df[FeatureFieldNH + FeatureFieldOH + FeatureFieldB + NumberFields + [\"LoanAmount\", \"LoanDuration\", \"LoanApproved\", \"CreditScore\", \"RiskScore\"]].head(20000)\n",
    "\n",
    "# to derive X for training\n",
    "# exclude Outliers records, \n",
    "# removed the real LoanApproved indicator and CreditScore, RiskScore\n",
    "X = df_train.drop([\"LoanApproved\", \"CreditScore\", \"RiskScore\"], axis=1)\n",
    "\n",
    "# to standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# To apply GaussianMixture clustering, for two clusters only, approve/reject\n",
    "gmm = None\n",
    "best_score = -1\n",
    "best_labels = None\n",
    "best_model = None\n",
    "label_swap_indicator = False\n",
    "for r in range(40,60):\n",
    "    # to predict the value and train the model\n",
    "    gmm = GaussianMixture(n_components=2, init_params=\"random\", random_state=r)\n",
    "    labels = gmm.fit_predict(X_scaled)\n",
    "\n",
    "    # to evaluate the picking of cluster with two assumptions\n",
    "    # 1. the False is more than True, ie. Approve is less than Reject\n",
    "    # 2. the gap between False and True, bigger is better, ie. lesser Approve is better\n",
    "    b = pd.DataFrame()\n",
    "    b[\"LoanApprovedAI\"] = labels.astype('bool')\n",
    "    t = b[\"LoanApprovedAI\"].value_counts()[True]\n",
    "    f = b[\"LoanApprovedAI\"].value_counts()[False]\n",
    "    score = abs(t - f)/(t+f)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_model = gmm\n",
    "        if (t < f): # Approve is less than Reject\n",
    "            best_labels = b[\"LoanApprovedAI\"]\n",
    "            label_swap_indicator = False\n",
    "            print(f\"best random_state={r} -> score={score}, (t={t}/f={f})\")\n",
    "        else:\n",
    "            label_swap_indicator = True\n",
    "            print(f\"best random_state={r} -> score={score}, (f={f}/t={t})\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"random_state={r} -> score={score}/({t}/{f})\")\n",
    "\n",
    "# Predict the component labels for each data point\n",
    "\n",
    "# Print the means and covariances of the components\n",
    "print(\"Means:\", gmm.means_)\n",
    "print(\"Covariances:\", gmm.covariances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logic before remove score calculation\n",
    "\n",
    "\n",
    "# to use unsupervised Gaussian Mixture method to train the model\n",
    "\n",
    "# to clone Y from training data, Y is used for result evaluation later\n",
    "Y = df_train.copy(deep=True)\n",
    "# to exclude Outlier data\n",
    "# Y = Y[Y[\"Outlier\"] == False]\n",
    "\n",
    "# to derive X for training\n",
    "X = Y.copy(deep=True)\n",
    "\n",
    "# to remove the manual label LoanApproved indicator and CreditScore, RiskScore, it may affect the training\n",
    "drop_fields = [\"LoanApproved\", \"CreditScore\", \"RiskScore\"]\n",
    "# to remove fields that are used to calculate other field. it may be redundant for training.\n",
    "drop_fields = drop_fields + [\"AnnualIncome\", \"TotalAssets\", \"TotalLiabilities\", \"LoanAmount\", ]\n",
    "drop_fields = drop_fields + [\"SavingsAccountBalance\", \"CheckingAccountBalance\", ]\n",
    "# drop_fields = drop_fields + [\"MonthlyIncome\", \"MonthlyDebtPayments\", \"MonthlyLoanPayment\", ]\n",
    "drop_fields = drop_fields + [\"Outlier\"]\n",
    "X = X.drop(drop_fields, axis=1)\n",
    "\n",
    "# to standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index) # to align the index\n",
    "\n",
    "# to train the model and predict the cluster\n",
    "best_model = None\n",
    "best_labels = None\n",
    "for e in range(1, 2):\n",
    "    # to calc the Seed Record that is approved for sure. It is used for training to identify which cluster is marked as approved.\n",
    "    LoanApprovedSeed = X.nlargest(64, \"NetWorth\").nlargest(16, \"MonthlyIncome\").nsmallest(4, \"MonthlyDebtPayments\").nsmallest(1, \"MonthlyLoanPayment\")\n",
    "\n",
    "    best_score = -1\n",
    "    for r in range(48,49):\n",
    "        # To use GaussianMixture for clustering, approve/reject\n",
    "        gmm = GaussianMixture(n_components=4, init_params=\"random\", random_state=r)\n",
    "\n",
    "        # to standardize the labels\n",
    "        labels = gmm.fit_predict(X_scaled)\n",
    "        b = pd.DataFrame({'LoanApprovedAIl' + str(e): labels}, index=X_scaled.index)\n",
    "        # to mark AI Loan Approved indicator as True if the label is same as the label of the seed record\n",
    "        b[\"LoanApprovedAI\" + str(e)] = b[\"LoanApprovedAIl\" + str(e)] == b.iat[LoanApprovedSeed.index[0], 0]\n",
    "\n",
    "        # to evaluate the picking of cluster\n",
    "        t = b[\"LoanApprovedAI\" + str(e)].value_counts()[True]\n",
    "        f = b[\"LoanApprovedAI\" + str(e)].value_counts()[False]\n",
    "        score = abs(t - f)/(t + f) # Balance Ratio, assume the gap between False and True, bigger is better, ie. lesser Approve is better\n",
    "        score = -abs(t - f)/(t + f) # Negative Balance Ratio, assume the gap between False and True, smaller is better, ie. Approve and Reject is similar\n",
    "        score = min(t, f) / max(t, f) # Class Balance Measure, assume Approve and Reject is similar\n",
    "        score = 1 - (t/(t + f))**2 - (f/(t + f))**2 # Gini Coefficient, assume Approve and Reject is similar\n",
    "        if score > best_score and t < f: \n",
    "            best_model = gmm\n",
    "            best_labels = b\n",
    "            best_score = score\n",
    "            print(f\"best epoch={e}, random_state={r}, Score={score}, True={t}, False={f}, SeedIndex={LoanApprovedSeed.index[0]}, SeedLabel={b.iat[LoanApprovedSeed.index[0], 0]}\")\n",
    "        else:\n",
    "            print(f\"epoch={e}, random_state={r}, Score={score}, True={t}, False={f}, SeedIndex={LoanApprovedSeed.index[0]}, SeedLabel={b.iat[LoanApprovedSeed.index[0], 0]}\")\n",
    "\n",
    "        b.value_counts()\n",
    "\n",
    "    Y[\"LoanApprovedAI\" + str(e)] = best_labels[\"LoanApprovedAI\" + str(e)]\n",
    "    Y[\"LoanApprovedAIl\" + str(e)] = best_labels[\"LoanApprovedAIl\" + str(e)]\n",
    "\n",
    "# Predict the component labels for each data point\n",
    "\n",
    "# Print the means and covariances of the components\n",
    "# print(\"Means:\", gmm.means_)\n",
    "# print(\"Covariances:\", gmm.covariances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logic to use repeat prediction for same set of data. result is fixed and no improvement.\n",
    "\n",
    "# to use unsupervised Gaussian Mixture method to train the model\n",
    "\n",
    "# to clone Y from training data, Y is used for result evaluation later\n",
    "Y = df_train.copy(deep=True).head(20000)\n",
    "Y[\"LoanApprovedAI\"] = False\n",
    "\n",
    "# to exclude Outlier data\n",
    "Y = Y[Y[\"Outlier\"] == False]\n",
    "\n",
    "# to derive X for training\n",
    "X = Y.copy(deep=True)\n",
    "\n",
    "# to remove the manual label LoanApproved indicator and CreditScore, RiskScore, it may affect the training\n",
    "drop_fields = [\"LoanApproved\", \"CreditScore\", \"RiskScore\"]\n",
    "# to remove fields that are used to calculate other field. it may be redundant for training.\n",
    "drop_fields = drop_fields + [\"AnnualIncome\", \"TotalAssets\", \"TotalLiabilities\", \"LoanAmount\", ]\n",
    "drop_fields = drop_fields + [\"SavingsAccountBalance\", \"CheckingAccountBalance\", ]\n",
    "# drop_fields = drop_fields + [\"MonthlyIncome\", \"MonthlyDebtPayments\", \"MonthlyLoanPayment\", ]\n",
    "drop_fields = drop_fields + [\"Outlier\"]\n",
    "X = X.drop(drop_fields, axis=1)\n",
    "\n",
    "# to train the model and predict the cluster\n",
    "# to define hyper-parameter\n",
    "hp_r = 48 # random_state\n",
    "hp_c = 6 # number of cluster for labeling\n",
    "hp_e = 5 # epoch for training\n",
    "hp_s = 0.2 # evaluation score\n",
    "\n",
    "# to standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index) # to align the index\n",
    "\n",
    "# To use GaussianMixture model for clustering, approve/reject\n",
    "gmm = GaussianMixture(n_components=hp_c, init_params=\"random\", random_state=hp_r)\n",
    "    \n",
    "best_model = None\n",
    "best_labels = None\n",
    "for e in range(1, hp_e + 1):\n",
    "    # to prepare X scaled of the epoch\n",
    "    # a = Y[Y[\"LoanApprovedAI\"] == False].index\n",
    "    Xe_scaled = X_scaled[X_scaled.index.isin(Y[Y[\"LoanApprovedAI\"] == False].index)]\n",
    "\n",
    "    # to calc the Seed Record that is approved for sure. It is used to identify which cluster is marked as approved during un-supervised training, semi-un-supervised\n",
    "    LoanApprovedSeed = Y[Y[\"LoanApprovedAI\"] == False].nlargest(64, \"NetWorth\").nlargest(16, \"MonthlyIncome\").nsmallest(4, \"MonthlyDebtPayments\").nsmallest(1, \"MonthlyLoanPayment\")\n",
    "\n",
    "    # to standardize the labels\n",
    "    labels = gmm.fit_predict(Xe_scaled)\n",
    "    b = pd.DataFrame({\"LoanApprovedAIl\" + str(e): labels}, index=Xe_scaled.index)\n",
    "    b[\"LoanApprovedAI\" + str(e)] = b[\"LoanApprovedAIl\" + str(e)] == b.loc[LoanApprovedSeed.index[0]][0]\n",
    "\n",
    "    # to evaluate the picking of cluster\n",
    "    t = b[\"LoanApprovedAI\" + str(e)].value_counts()[True]\n",
    "    f = b[\"LoanApprovedAI\" + str(e)].value_counts()[False]\n",
    "    score = -abs(t - f)/(t + f) # Negative Balance Ratio, assume the gap between False and True, smaller is better, ie. Approve and Reject is similar\n",
    "    score = 1 - (t/(t + f))**2 - (f/(t + f))**2 # Gini Coefficient, assume Approve and Reject is similar\n",
    "    score = abs(t - f)/(t + f) # Balance Ratio, assume the gap between False and True, bigger is better, ie. lesser Approve is better\n",
    "    score = min(t, f) / max(t, f) # Class Balance Measure, assume Approve and Reject is similar\n",
    "    if score > hp_s: \n",
    "        print(f\"skip, epoch={e}, random_state={hp_r}, Score={score}, True={t}, False={f}\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"epoch={e}, random_state={hp_r}, Score={score}, True={t}, False={f}\")\n",
    "        \n",
    "    # to keep the label\n",
    "    Y[\"LoanApprovedAIl\" + str(e)] = b[\"LoanApprovedAIl\" + str(e)]\n",
    "    # to mark LoanApprovedAI of current epoch as True if the label is same as the label of the seed record\n",
    "    Y[\"LoanApprovedAI\" + str(e)] = b[\"LoanApprovedAI\" + str(e)]\n",
    "\n",
    "    # to mark the overall LoanApprovedAI as True if any of epoch mark the it as Approved\n",
    "    col_approved =[]\n",
    "    for i in range(1, e + 1):\n",
    "        col_approved.append(\"LoanApprovedAI\" + str(i))\n",
    "    Y[\"LoanApprovedAI\"] = Y[col_approved].any(axis=1)\n",
    "\n",
    "    print(f\"epoch={e}, random_state={hp_r}, SeedIndex={LoanApprovedSeed.index[0]}, SeedLabel={b.loc[LoanApprovedSeed.index[0]][0]}\")\n",
    "    b.value_counts()\n",
    "    df_group = Y.groupby([\"LoanApprovedAI\", \"LoanApproved\"]).agg({'MonthlyLoanPayment': ['sum', 'count']})\n",
    "    formatted_sum = df_group.applymap(lambda x: f\"{x:,.2f}\")\n",
    "    print(formatted_sum)\n",
    "\n",
    "# Predict the component labels for each data point\n",
    "\n",
    "# Print the means and covariances of the components\n",
    "# print(\"Means:\", gmm.means_)\n",
    "# print(\"Covariances:\", gmm.covariances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to draw static diagram\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(eval_result['Clusters'], eval_result['Random'], eval_result['Score'], \n",
    "           s=eval_result['Accuracy']*1000,  # Scale Accuracy to suitable dot size\n",
    "           alpha=0.7, edgecolors='black')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Clusters')\n",
    "ax.set_ylabel('Random')\n",
    "ax.set_zlabel('Score')\n",
    "ax.set_title('3D Scatter Plot of Clusters, Random, and Score')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories\n",
    "categories = ['TT', 'FF', 'TF', 'FT']\n",
    "N = len(categories)\n",
    "\n",
    "# Angles\n",
    "angles = np.linspace(0, 2*np.pi, N, endpoint=False)\n",
    "\n",
    "# Create radar chart\n",
    "fig = go.Figure()\n",
    "for i, cluster in enumerate(data['Clusters'].unique()):\n",
    "    values = data[data['Clusters'] == cluster][['TT', 'FF', 'TF', 'FT']].mean().values\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=values,\n",
    "        theta=categories,\n",
    "        name=f'Cluster {cluster}',\n",
    "        line=dict(color=f'rgb({i*50}, {i*20}, {i*100})')\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Radar Chart',\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "            visible=True,\n",
    "            range=[0, data[['TT', 'FF', 'TF', 'FT']].max().max()]\n",
    "        )\n",
    "    ),\n",
    "    showlegend=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create radar chart\n",
    "categories = ['TT', 'FF', 'TF', 'FT']\n",
    "N = len(categories)\n",
    "\n",
    "# Plot radar chart\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, cluster in enumerate(data['Clusters'].unique()):\n",
    "    values = data[data['Clusters'] == cluster][['TT', 'FF', 'TF', 'FT']].mean().values\n",
    "    angles = np.linspace(0, 2*np.pi, N, endpoint=False)\n",
    "    plt.polar(angles, values, 'o-', linewidth=2, label=cluster)\n",
    "\n",
    "plt.thetagrids(angles * 180/np.pi, categories)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "conf_mat = data[['TT', 'FF', 'TF', 'FT']]\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_mat, annot=True, cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt data for plotting\n",
    "data_melt = data.melt(id_vars=['Clusters', 'Random'], value_vars=['TT', 'FF', 'TF', 'FT'])\n",
    "\n",
    "# Plot bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Clusters', y='Random', hue='Clusters', data=data_melt)\n",
    "plt.title('TT, FF, TF, FT Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to prepare evaluation records for visualization analysis\n",
    "C = pd.DataFrame()\n",
    "C[\"LoanApproved\"] = Y[\"LoanApproved\"]\n",
    "C[\"LoanApprovedAI\"] = b[\"LoanApprovedAI\"]\n",
    "Y[\"LoanApprovedCompare\"] = C[\"LoanApproved\"] == C[\"LoanApprovedAI\"]\n",
    "\n",
    "accuracy = C[\"LoanApprovedCompare\"].value_counts()[True]/(C[\"LoanApprovedCompare\"].value_counts()[True] + C[\"LoanApprovedCompare\"].value_counts()[False])\n",
    "print(f\"Accuracy={accuracy}\")\n",
    "\n",
    "la_tf = C.groupby([\"LoanApprovedAI\", \"LoanApproved\"]).size()\n",
    "eval_result = pd.concat([eval_result, pd.Series({\"Clusters\": c, \"Random\": r, \"Score\": score, \"Accuracy\": accuracy, \"TT\": la_tf.loc[(True,True)], \"FF\": la_tf.loc[(False,False)], \"TF\": la_tf.loc[(True,False)], \"FT\": la_tf.loc[(False,True)]}).to_frame().T], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot matrix\n",
    "sns.pairplot(data[['TT', 'FF', 'TF', 'FT']].head(1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to draw live diagram to show the most accuracy data\n",
    "max_acc_idx = eval_result['Accuracy'].idxmax()\n",
    "max_score_idx = eval_result['Score'].idxmax()\n",
    "\n",
    "# 3D scatter plot\n",
    "fig = go.Figure(data=[\n",
    "        go.Scatter3d(\n",
    "            x=eval_result['Clusters'],\n",
    "            y=eval_result['Random'],\n",
    "            z=eval_result['Score'],\n",
    "            mode='markers',\n",
    "            marker=dict(size=eval_result['Accuracy']*50, color='blue'),\n",
    "            hoverinfo='text',\n",
    "            hovertext=[f\"Accuracy: {acc:.2f}%\" for acc in eval_result['Accuracy']*100]),\n",
    "        go.Scatter3d(\n",
    "            x=[eval_result.loc[max_acc_idx, 'Clusters']],\n",
    "            y=[eval_result.loc[max_acc_idx, 'Random']],\n",
    "            z=[eval_result.loc[max_acc_idx, 'Score']],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=[eval_result.loc[max_acc_idx, 'Accuracy']*50], \n",
    "                color='orange', \n",
    "                symbol='circle',\n",
    "                line=dict(width=1)\n",
    "            ),\n",
    "            hoverinfo='text',\n",
    "            hovertext=[f\"Max Accuracy: {eval_result.loc[max_acc_idx, 'Accuracy']*100:.2f}%\"]),\n",
    "        go.Scatter3d(\n",
    "            x=[eval_result.loc[max_score_idx, 'Clusters']],\n",
    "            y=[eval_result.loc[max_score_idx, 'Random']],\n",
    "            z=[eval_result.loc[max_score_idx, 'Score']],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=[eval_result.loc[max_score_idx, 'Accuracy']*50], \n",
    "                color='green', \n",
    "                symbol='circle',\n",
    "                line=dict(width=1)\n",
    "            ),\n",
    "            hoverinfo='text',\n",
    "            hovertext=[f\"Accuracy of Max Score: {eval_result.loc[max_score_idx, 'Accuracy']*100:.2f}%\"])\n",
    "    ])\n",
    "\n",
    "# Set labels and title\n",
    "fig.update_layout(\n",
    "    title='Clusters, Random, Score and Accuracy',\n",
    "    scene=dict(\n",
    "        xaxis_title='Clusters',\n",
    "        xaxis=dict(tickformat=\"d\", dtick=1),\n",
    "        yaxis_title='Random',\n",
    "        yaxis=dict(tickformat=\"d\", dtick=1),\n",
    "        zaxis_title='Score'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show interactive plot\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To evaluate the result add cluster labels to the original DataFrame\n",
    "\n",
    "C1 = X1.copy(deep=True)\n",
    "C1['LoanApproved'] = C[\"LoanApproved\"]\n",
    "C1['LoanApprovedAI'] = best_labels1[\"LoanApprovedAI\"]\n",
    "C1[\"LoanApprovedCompare\"] = C1[\"LoanApproved\"] == C1[\"LoanApprovedAI\"]\n",
    "\n",
    "C1[\"LoanApprovedCompare\"].value_counts()\n",
    "C1[\"LoanApprovedCompare\"].value_counts()[True]/(C1[\"LoanApprovedCompare\"].value_counts()[True] + C1[\"LoanApprovedCompare\"].value_counts()[False])\n",
    "\n",
    "df_group = C1.groupby([\"LoanApprovedCompare\", \"LoanApprovedAI\", \"LoanApproved\"]).agg({'MonthlyLoanPayment': ['sum', 'count']})\n",
    "formatted_sum = df_group.applymap(lambda x: f\"{x:,.2f}\")\n",
    "print(formatted_sum)\n",
    "\n",
    "C1.iloc[LoanApprovedSeed.index[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find out the relation between features and labels\n",
    "\n",
    "# Calculate silhouette score\n",
    "# silhouette_avg = silhouette_score(C, best_labels)\n",
    "# print(\"Silhouette score:\", silhouette_avg)\n",
    "\n",
    "# Calculate correlation coefficients\n",
    "# correlations = C.corrwith(pd.Series(best_labels))\n",
    "# print(\"Correlations:\")\n",
    "# print(correlations)\n",
    "\n",
    "# Create box plots\n",
    "# sns.boxplot(x=labels, y='Age', data=C)\n",
    "# plt.title('Age 1 vs. Cluster')\n",
    "# plt.show()\n",
    "\n",
    "# Create scatter plots\n",
    "# sns.scatterplot(x='TotalAssets', y='EmploymentStatus_Employed', hue=C['LoanApproved'], data=C)\n",
    "# plt.title('EmploymentStatus_Employed 1 vs. TotalAssets 2 by Cluster')\n",
    "# plt.show()\n",
    "\n",
    "# sns.pairplot(df_train[[\"MonthlyLoanPayment\", \"TotalAssets\", \"TotalLiabilities\", \"MonthlyDebtPayments\", \"AnnualIncome\"] + ['LoanApproved']], hue='LoanApproved', diag_kind='kde', palette='viridis', markers=['o', 'o', 'o'])\n",
    "# plt.show()\n",
    "\n",
    "# sns.factorplot()\n",
    "\n",
    "# to fine out what are the fields that may affect the LoadApproved\n",
    "# to find out what are the fields that may affect CreditScore\n",
    "# to find out what are the fields that may affect RiskScore\n",
    "# to find out what are the fields that may affect the BaseInterestRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to analysis C DataFrame, to find out what is pattern for discrepancy between the real result and AI result.\n",
    "CorrFields = [\n",
    "    \"TotalDebtToIncomeRatio\", \"BankruptcyHistory\", \"NetWorth\", \"PreviousLoanDefaults\", \\\n",
    "    # \"LoanAmount\", \"TotalAssets\", \"TotalLiabilities\", \"MonthlyDebtPayments\", \"AnnualIncome\", \\\n",
    "              ]\n",
    "df_false = C[C[\"LoanApprovedCompare\"] == False][C[\"LoanApprovedAI\"] == True]\n",
    "\n",
    "# to show all fields correlation\n",
    "correlation_matrix = df_false[CorrFields].corr()\n",
    "\n",
    "threshold = 0.01\n",
    "mask = abs(correlation_matrix) > threshold\n",
    "correlation_matrix_filtered = correlation_matrix.where(mask, np.nan)\n",
    "\n",
    "fz = len(CorrFields) * 2\n",
    "plt.figure(figsize=(fz, fz))\n",
    "sns.heatmap(correlation_matrix_filtered, annot=True, cmap='viridis', linewidths=0.5)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('AI Approved by Manual Rejected Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "new_scaler = load('credit-risk-scaler.joblib')\n",
    "try:\n",
    "    check_is_fitted(new_scaler)  # This will raise an error if not fitted\n",
    "except :\n",
    "    print(\"Scaler is not fitted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Processing\n",
    "\n",
    "# object fields using factorize encoding\n",
    "Best Score=0.20092621999999993, cluster=6, random_state=42, True=2266, False=17734, SeedIndex=9567, SeedLabel=2\n",
    "Accuracy=0.7159\n",
    "Clusters        6.000000\n",
    "Random         42.000000\n",
    "Score           0.200926\n",
    "AT           2266.000000\n",
    "AF          17734.000000\n",
    "Accuracy        0.715900\n",
    "TT            682.000000\n",
    "FF          13636.000000\n",
    "TF           1584.000000\n",
    "FT           4098.000000\n",
    "\n",
    "# object fields using one-hot encoding\n",
    "Best Score=0.18818695499999993, cluster=6, random_state=42, True=2103, False=17897, SeedIndex=9567, SeedLabel=4\n",
    "Accuracy=0.70425\n",
    "Clusters        6.000000\n",
    "Random         42.000000\n",
    "Score           0.188187\n",
    "AT           2103.000000\n",
    "AF          17897.000000\n",
    "Accuracy        0.704250\n",
    "TT            484.000000\n",
    "FF          13601.000000\n",
    "TF           1619.000000\n",
    "FT           4296.000000\n",
    "\n",
    "# number fields using bin + one-hot encoding\n",
    "Best Score=0.02556459500000008, cluster=6, random_state=42, True=259, False=19741, SeedIndex=9567, SeedLabel=4\n",
    "Accuracy=0.75685\n",
    "Clusters        6.000000\n",
    "Random         42.000000\n",
    "Score           0.025565\n",
    "AT            259.000000\n",
    "AF          19741.000000\n",
    "Accuracy        0.756850\n",
    "TT             88.000000\n",
    "FF          15049.000000\n",
    "TF            171.000000\n",
    "FT           4692.000000\n",
    "\n",
    "# removed some fields1\n",
    "Best Score=0.15130399500000014, cluster=6, random_state=42, True=1649, False=18351, SeedIndex=9567, SeedLabel=4\n",
    "Accuracy=0.71715\n",
    "Clusters        6.000000\n",
    "Random         42.000000\n",
    "Score           0.151304\n",
    "AT           1649.000000\n",
    "AF          18351.000000\n",
    "Accuracy        0.717150\n",
    "TT            386.000000\n",
    "FF          13957.000000\n",
    "TF           1263.000000\n",
    "FT           4394.000000\n",
    "\n",
    "# removed calculated field\n",
    "Best Score=0.1404479999999999, cluster=6, random_state=42, True=1520, False=18480, SeedIndex=9567, SeedLabel=4\n",
    "Accuracy=0.7217\n",
    "Clusters        6.000000\n",
    "Random         42.000000\n",
    "Score           0.140448\n",
    "AT           1520.000000\n",
    "AF          18480.000000\n",
    "Accuracy        0.721700\n",
    "TT            367.000000\n",
    "FF          14067.000000\n",
    "TF           1153.000000\n",
    "FT           4413.000000\n",
    "# removed original fields\n",
    "Best Score=0.22915200000000002, cluster=6, random_state=42, True=2640, False=17360, SeedIndex=9567, SeedLabel=2\n",
    "Accuracy=0.6755\n",
    "Clusters        6.000000\n",
    "Random         42.000000\n",
    "Score           0.229152\n",
    "AT           2640.000000\n",
    "AF          17360.000000\n",
    "Accuracy        0.675500\n",
    "TT            465.000000\n",
    "FF          13045.000000\n",
    "TF           2175.000000\n",
    "FT           4315.000000\n",
    "\n",
    "# to use kmean++\n",
    "Best Score=0.20285659500000008, cluster=6, random_state=42, True=2291, False=17709, SeedIndex=9567, SeedLabel=4\n",
    "Accuracy=0.70525\n",
    "Clusters        6.000000\n",
    "Random         42.000000\n",
    "Score           0.202857\n",
    "AT           2291.000000\n",
    "AF          17709.000000\n",
    "Accuracy        0.705250\n",
    "TT            588.000000\n",
    "FF          13517.000000\n",
    "TF           1703.000000\n",
    "FT           4192.000000\n",
    "\n",
    "# to use silhouette_score to evaluate\n",
    "Best Score=-0.00399347717175795, cluster=6, random_state=42, True=2640, False=17360, SeedIndex=9567, SeedLabel=2\n",
    "Accuracy=0.6755\n",
    "Clusters        6.000000\n",
    "Random         42.000000\n",
    "Score          -0.003993\n",
    "AT           2640.000000\n",
    "AF          17360.000000\n",
    "Accuracy        0.675500\n",
    "TT            465.000000\n",
    "FF          13045.000000\n",
    "TF           2175.000000\n",
    "FT           4315.000000\n",
    "\n",
    "## final best result with all above tuning\n",
    "Best Score=0.45614964715122097, cluster=2, random_state=43, True=1793, False=18207, SeedIndex=9567, SeedLabel=1\n",
    "Accuracy=0.74435\n",
    "Clusters        2.00000\n",
    "Random         43.00000\n",
    "Score           0.45615\n",
    "AT           1793.00000\n",
    "AF          18207.00000\n",
    "Accuracy        0.74435\n",
    "TT            730.00000\n",
    "FF          14157.00000\n",
    "TF           1063.00000\n",
    "FT           4050.00000\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
